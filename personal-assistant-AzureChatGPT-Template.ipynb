{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing conversation history with the ChatGPT model\n",
    "This sample notebook demonstrates a couple of simple patterns you can use for managing the prompts and conversation history with the ChatGPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://contosomikachatgpt.openai.azure.com/\"\n",
    "#openai.api_base = os.getenv(\"GPT_API_ENDPOINT\")\n",
    "openai.api_version = \"2022-12-01\"\n",
    "#openai.api_key = os.getenv('api_key')\n",
    "openai.api_key = \"6c213d9c6e9143ed8502cf4f4880f751\"\n",
    "\n",
    "# The name of your deployment of the ChatGPT model\n",
    "chatgpt_model_name = \"gpt-35-turbo\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Create the system message for ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a marketing writing assistant. You help come up with creative content ideas and content like marketing emails, blog posts, tweets, ad copy, listicles, product FAQs, and product descriptions. \n",
      "You write in a friendly yet professional tone and you can tailor your writing style that best works for a user-specified audience. \n",
      "\n",
      "Additional instructions:\n",
      "- Make sure you understand your user's audience so you can best write the content.\n",
      "- Ask clarifying questions when you need additional information. Examples include asking about the audience or medium for the content.\n",
      "- Don't write any content that could be harmful.\n",
      "- Don't write any content that could be offensive or inappropriate.\n",
      "- Don't write any content that speaks poorly of any product or company.\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "base_system_message = \"\"\"\n",
    "You are a marketing writing assistant. You help come up with creative content ideas and content like marketing emails, blog posts, tweets, ad copy, listicles, product FAQs, and product descriptions. \n",
    "You write in a friendly yet professional tone and you can tailor your writing style that best works for a user-specified audience. \n",
    "\n",
    "Additional instructions:\n",
    "- Make sure you understand your user's audience so you can best write the content.\n",
    "- Ask clarifying questions when you need additional information. Examples include asking about the audience or medium for the content.\n",
    "- Don't write any content that could be harmful.\n",
    "- Don't write any content that could be offensive or inappropriate.\n",
    "- Don't write any content that speaks poorly of any product or company.\n",
    "\"\"\"\n",
    "\n",
    "system_message = f\"<|im_start|>system\\n{base_system_message.strip()}\\n<|im_end|>\"\n",
    "print(system_message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Define helper functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to create the prompt from the system message and the messages\n",
    "# The function assumes `messages` is a list of dictionaries with `sender` and `text` keys\n",
    "# Example: messages = [{\"sender\": \"user\", \"text\": \"I want to write a blog post about my company.\"}]\n",
    "def create_prompt(system_message, messages):\n",
    "    prompt = system_message\n",
    "    for message in messages:\n",
    "        prompt += f\"\\n<|im_start|>{message['sender']}\\n{ message['text']}\\n<|im_end|>\"\n",
    "    prompt += \"\\n<|im_start|>assistant\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 152\n"
     ]
    }
   ],
   "source": [
    "import tiktoken \n",
    "\n",
    "# Defining a function to estimate the number of tokens in a prompt\n",
    "def estimate_tokens(prompt):\n",
    "    cl100k_base = tiktoken.get_encoding(\"cl100k_base\") \n",
    "\n",
    "    enc = tiktoken.Encoding( \n",
    "        name=\"chatgpt\",  \n",
    "        pat_str=cl100k_base._pat_str, \n",
    "        mergeable_ranks=cl100k_base._mergeable_ranks, \n",
    "        special_tokens={ \n",
    "            **cl100k_base._special_tokens, \n",
    "            \"<|im_start|>\": 100264, \n",
    "            \"<|im_end|>\": 100265\n",
    "        } \n",
    "    ) \n",
    "\n",
    "    tokens = enc.encode(prompt,  allowed_special={\"<|im_start|>\", \"<|im_end|>\"})\n",
    "    return len(tokens)\n",
    "\n",
    "# Estimate the number of tokens in the system message. Tokens in the system message will be sent in every request.\n",
    "token_count = estimate_tokens(system_message)\n",
    "print(\"Token count: {}\".format(token_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to send the prompt to the ChatGPT model\n",
    "def send_message(prompt, model_name, max_response_tokens=500):\n",
    "    response = openai.Completion.create(\n",
    "        engine=chatgpt_model_name,\n",
    "        prompt=prompt,\n",
    "        temperature=0.5,\n",
    "        max_tokens=max_response_tokens,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=['<|im_end|>']\n",
    "    )\n",
    "    return response['choices'][0]['text'].strip()\n",
    "\n",
    "# Defining a function to print out the conversation in a readable format\n",
    "def print_conversation(messages):\n",
    "    for message in messages:\n",
    "        print(f\"[{message['sender'].upper()}]\")\n",
    "        print(message['text'])\n",
    "        print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Start the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the first message that will be sent to the model. Feel free to update this.\n",
    "user_message = \"I want to write a blog post about the impact of AI on the future of work.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a marketing writing assistant. You help come up with creative content ideas and content like marketing emails, blog posts, tweets, ad copy, listicles, product FAQs, and product descriptions. \n",
      "You write in a friendly yet professional tone and you can tailor your writing style that best works for a user-specified audience. \n",
      "\n",
      "Additional instructions:\n",
      "- Make sure you understand your user's audience so you can best write the content.\n",
      "- Ask clarifying questions when you need additional information. Examples include asking about the audience or medium for the content.\n",
      "- Don't write any content that could be harmful.\n",
      "- Don't write any content that could be offensive or inappropriate.\n",
      "- Don't write any content that speaks poorly of any product or company.\n",
      "<|im_end|>\n",
      "<|im_start|>user\n",
      "I want to write a blog post about the impact of AI on the future of work.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the list of messages. Sender can be either \"user\" or \"assistant\"\n",
    "messages = [{\"sender\": \"user\", \"text\": user_message}]\n",
    "\n",
    "# Create the full prompt\n",
    "prompt = create_prompt(system_message, messages)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 179\n"
     ]
    }
   ],
   "source": [
    "token_count = estimate_tokens(prompt)\n",
    "print(f\"Token count: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mika\\OneDrive - Microsoft\\Documents\\GitHub\\AI-Personal-Assistant\\personal-assistant-AzureChatGPT-Template.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m max_response_tokens \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m response \u001b[39m=\u001b[39m send_message(prompt, chatgpt_model_name, max_response_tokens)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m messages\u001b[39m.\u001b[39mappend({\u001b[39m\"\u001b[39m\u001b[39msender\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: response})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m print_conversation(messages)\n",
      "\u001b[1;32mc:\\Users\\Mika\\OneDrive - Microsoft\\Documents\\GitHub\\AI-Personal-Assistant\\personal-assistant-AzureChatGPT-Template.ipynb Cell 13\u001b[0m in \u001b[0;36msend_message\u001b[1;34m(prompt, model_name, max_response_tokens)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_message\u001b[39m(prompt, model_name, max_response_tokens\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         engine\u001b[39m=\u001b[39;49mchatgpt_model_name,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_response_tokens,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         top_p\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         stop\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m<|im_end|>\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Mika/OneDrive%20-%20Microsoft/Documents/GitHub/AI-Personal-Assistant/personal-assistant-AzureChatGPT-Template.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\Mika\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\Mika\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\Mika\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py:227\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    207\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    208\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    216\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    217\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    218\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    219\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    226\u001b[0m     )\n\u001b[1;32m--> 227\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    228\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\Mika\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py:620\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    613\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    614\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    615\u001b[0m         )\n\u001b[0;32m    616\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    617\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    619\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 620\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    623\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    624\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    625\u001b[0m         ),\n\u001b[0;32m    626\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    627\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Mika\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py:680\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    678\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    679\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    681\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    682\u001b[0m     )\n\u001b[0;32m    683\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes, please wait a moment and try again."
     ]
    }
   ],
   "source": [
    "max_response_tokens = 500\n",
    "\n",
    "response = send_message(prompt, chatgpt_model_name, max_response_tokens)\n",
    "messages.append({\"sender\": \"assistant\", \"text\": response})\n",
    "\n",
    "print_conversation(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Continue the conversation\n",
    "\n",
    "When working with the ChatGPT model, it's your responsibity to make sure you stay within the token limits of the model. The model can handle a maximum of 4096 tokens, and this includes the number of tokens in the prompt as well as the `max_tokens` you're requesting from the model. If you exceed these limits, the model will return an error.\n",
    "\n",
    "You should also consider the trade-off between maintaining more of the conversation history and the cost/latency that you'll incur by including those tokens in the prompt. Shorter prompts are cheaper and faster. The amount of the previous conversation you include also makes a difference in how the model responds.\n",
    "\n",
    "In this notebook, we'll show two strategies for managing the conversation history when working with the ChatGPT model.\n",
    "- Option 1: Keep the conversation within a given token limit\n",
    "- Option 2: Keep the conversation within a given number of turns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Keep the conversation within a given token limit\n",
    "\n",
    "`overall_max_tokens` is the maximum number of tokens that you want to include in the prompt. Th maximum number this can be set to is 4096 but you can also consider reducing this number to reduce the cost and latency of the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_max_tokens = 4096\n",
    "prompt_max_tokens = overall_max_tokens - max_response_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue the conversation below by editing the user_message and running the cell as many times as you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 263\n",
      "[USER]\n",
      "I want to write a blog post about the impact of AI on the future of work.\n",
      "\n",
      "[ASSISTANT]\n",
      "Sure! Before we begin, can you tell me a bit more about the audience you're targeting with this blog post? Are they tech-savvy professionals or general readers with little to no knowledge about AI? Understanding your audience will help me tailor the content to their level of understanding.\n",
      "\n",
      "[USER]\n",
      "The target audience for the blog post should be business leaders working in the tech industry.\n",
      "\n",
      "[ASSISTANT]\n",
      "Great! Here's a rough outline for a blog post on the impact of AI on the future of work:\n",
      "\n",
      "1. Introduction: Briefly explain what AI is and how it's currently being used in the tech industry.\n",
      "\n",
      "2. The good: Discuss the potential benefits of AI, such as increased efficiency and productivity, cost savings, and improved decision-making.\n",
      "\n",
      "3. The bad: Discuss the potential downsides of AI, such as job displacement and the need for retraining.\n",
      "\n",
      "4. The future: Discuss how AI is expected to shape the future of work, including the creation of new job opportunities and the need for a new skillset.\n",
      "\n",
      "5. Conclusion: Sum up the main points and discuss the importance of preparing for the future of work with AI.\n",
      "\n",
      "Let me know if you have any questions or if there's anything else you'd like me to include in the post.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_message = \"The target audience for the blog post should be business leaders working in the tech industry.\"\n",
    "#user_message = \"Let's talk about generative AI and keep the tone informational but also friendly.\"\n",
    "#user_message = \"Show me a few more examples\"\n",
    "messages.append({\"sender\": \"user\", \"text\": user_message})\n",
    "\n",
    "prompt = create_prompt(system_message, messages)\n",
    "token_count = estimate_tokens(prompt)\n",
    "print(f\"Token count: {token_count}\")\n",
    "\n",
    "# remove first message while over the token limit\n",
    "while token_count > prompt_max_tokens:\n",
    "    messages.pop(0)\n",
    "    prompt = create_prompt(system_message, messages)\n",
    "    token_count = estimate_tokens(prompt)\n",
    "\n",
    "response = send_message(prompt, chatgpt_model_name, max_response_tokens)\n",
    "\n",
    "messages.append({\"sender\": \"assistant\", \"text\": response})\n",
    "print_conversation(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Keep the conversation within a given number of turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_messages = 10\n",
    "\n",
    "overall_max_tokens = 4096\n",
    "prompt_max_tokens = overall_max_tokens - max_response_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can continue the conversation below by editing the user_message and running the cell as many times as you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"Make the post about generative AI aimed at business leaders who have some knowledge of the technology.\"\n",
    "messages.append({\"sender\": \"user\", \"text\": user_message})\n",
    "\n",
    "while len(messages) > max_messages:\n",
    "    messages.pop(0)\n",
    "\n",
    "prompt = create_prompt(system_message, messages)\n",
    "token_count = estimate_tokens(prompt)\n",
    "\n",
    "while token_count > prompt_max_tokens:\n",
    "    # remove first message from messages\n",
    "    messages.pop(0)\n",
    "    prompt = create_prompt(system_message, messages, max_response_tokens)\n",
    "    token_count = estimate_tokens(prompt)\n",
    "\n",
    "response = send_message(prompt, chatgpt_model_name)\n",
    "messages.append({\"sender\": \"assistant\", \"text\": response})\n",
    "# print_conversation(messages)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc180f703c9255d3d630e6d09ed4eb3355d27845db546035ce1b410f2bfa43b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
